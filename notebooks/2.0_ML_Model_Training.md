2.0 ML Model Training and Data Preparation Pipeline

This notebook documents the steps taken to process the raw automobile sales data, prepare features for the Machine Learning model, and train the final classification model used in the Dash application.

The goal of the model is to classify sales as either "Successful" or "Unsuccessful" based on historical features.

1. Setup and Data Loading

First, we load the necessary libraries and the raw data that was simulated during the data acquisition phase.

import pandas as pd
import numpy as np
import pickle
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Define file paths
RAW_DATA_PATH = '../data/raw/auto_sales_raw.csv'
PROCESSED_DATA_PATH = '../data/processed/cleaned_data.csv'
MODEL_PATH = '../data/model/classification_model.pkl'

# Load the raw data file
try:
    df = pd.read_csv(RAW_DATA_PATH)
    print(f"Data loaded successfully from {RAW_DATA_PATH}")
except FileNotFoundError:
    print("ERROR: Raw data file not found. Please run the setup_pipeline_data.py script.")
    exit()

# Display initial data structure
print("\nInitial Data Head:")
print(df.head())


2. Data Cleaning and Feature Engineering

We prepare the categorical and numerical features for the model.

# Convert Price to a numerical type, removing 'K' if necessary (though mock data is clean)
df['Price_k'] = df['Price_k'].astype(float)

# --- Feature Engineering ---
# 1. Encode categorical features (Manufacturer, Region, TimePeriod)
# We use LabelEncoder for simplicity since Logistic Regression can handle ordinal inputs.
le = LabelEncoder()
df['Manufacturer_Encoded'] = le.fit_transform(df['Manufacturer'])
df['Region_Encoded'] = le.fit_transform(df['Region'])
df['TimePeriod_Encoded'] = le.fit_transform(df['TimePeriod'])

# 2. Encode the target variable (Success_Category)
df['Success_Target'] = df['Success_Category'].apply(
    lambda x: 1 if x == 'Successful Sales' else 0
)

# Define the final DataFrame used by the dashboard
cleaned_df = df[[
    'Manufacturer', 'Region', 'TimePeriod', 'SalesVolume', 
    'Price_k', 'Sales_Category', 'Success_Target'
]]

print("\nFeatures Engineered and Ready for Modeling.")


3. Training the Classification Model

We split the data and train a Logistic Regression model to classify sales success.

# Define Features (X) and Target (y)
features = ['SalesVolume', 'Price_k', 'Manufacturer_Encoded', 'Region_Encoded', 'TimePeriod_Encoded']
X = df[features]
y = df['Success_Target']

# Split data for training and testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the Logistic Regression Model
model = LogisticRegression(solver='liblinear', random_state=42)
model.fit(X_train, y_train)

# Model Evaluation
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"\nModel Training Complete.")
print(f"Logistic Regression Model Accuracy: {accuracy:.4f}")


4. Saving Files for the Dash Application

The final, crucial step is saving the processed data and the trained model. The Dash application (app.py) will load these two files directly.

# Save the cleaned DataFrame to the processed data folder
cleaned_df.to_csv(PROCESSED_DATA_PATH, index=False)
print(f"\nCleaned data saved to: {PROCESSED_DATA_PATH}")

# Save the trained model using pickle
with open(MODEL_PATH, 'wb') as file:
    pickle.dump(model, file)
print(f"Trained ML Model saved to: {MODEL_PATH}")


This completes the documentation for the model creation and data pipeline steps.